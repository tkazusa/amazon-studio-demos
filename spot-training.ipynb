{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning with Keras on Amazon SageMaker\n",
    "\n",
    "Last update: December 3rd, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a modular, fully managed Machine Learning service that lets you easily build, train and deploy models at any scale.\n",
    "\n",
    "In this notebook, we'll use Keras (with the TensorFlow backend) to build a simple Convolutional Neural Network (CNN). We'll then train it to classify the Fashion-MNIST image data set. Fashion-MNIST is a Zalando dataset consisting of a training set of 60,000 examples and a validation set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: it's a drop-in replacement for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "  * Amazon SageMaker documentation [ https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html ]\n",
    "  * SageMaker SDK \n",
    "    * Code [ https://github.com/aws/sagemaker-python-sdk ] \n",
    "    * Documentation [ https://sagemaker.readthedocs.io/ ]\n",
    "  * Fashion-MNIST [ https://github.com/zalandoresearch/fashion-mnist ] \n",
    "  * Keras documentation [ https://keras.io/ ]\n",
    "  * Numpy documentation [ https://docs.scipy.org/doc/numpy/index.html ]\n",
    "  \n",
    "### https://gitlab.com/juliensimon/amazon-studio-demos\n",
    "### Twitter: @julsimon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the latest SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -q --upgrade pip\n",
    "pip install -q sagemaker smdebug-rulesconfig==0.1.2  keras pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data set from the Internet. Fortunately, Keras provides a simple way to do this. The data set is already split (training and validation), with separate Numpy arrays for samples and labels. \n",
    "\n",
    "We create a local directory, and save the training and validation data sets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "np.savez('./data/training', image=x_train, label=y_train)\n",
    "np.savez('./data/validation', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at our Keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_script_path = '/root/aim410/mnist_keras_tf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $keras_script_path\n",
    "pygmentize $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps are:\n",
    "  * receive and parse command line arguments: five hyper parameters, and four environment variables (we'll get back to these in a moment)\n",
    "  * load the data sets\n",
    "  * make sure data sets have the right shape for TensorFlow (channels last)\n",
    "  * normalize data sets, i.e. tranform [0-255] pixel values to [0-1] values\n",
    "  * one-hot encode category labels (not familiar with this? More info: [ https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ ])\n",
    "  * Build a Sequential model in Keras: two convolution block with max pooling, followed by a fully connected layer with dropout, and a final classification layer. Don't worry if this sounds like gibberish, it's not our focus today\n",
    "  * Train the model, leveraging multiple GPUs if they're available.\n",
    "  * Print statistics\n",
    "  * Save the model in TensorFlow serving format\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data set to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker training instances expect data sets to be stored in Amazon S3, so let's upload them there. We could use boto3 to do this, but the SageMaker SDK includes a simple function: [Session.upload_data()](https://sagemaker.readthedocs.io/en/stable/session.html).\n",
    "\n",
    "\n",
    "\n",
    "*Note: for high-performance workloads, Amazon EFS and Amazon FSx for Lustre are now also supported. More info [here](https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'keras-fashion-mnist'\n",
    "\n",
    "# Upload the training data set to 'keras-fashion-mnist/training'\n",
    "training_input_path   = sess.upload_data('data/training.npz', key_prefix=prefix+'/training')\n",
    "\n",
    "# Upload the validation data set to 'keras-fashion-mnist/validation'\n",
    "validation_input_path = sess.upload_data('data/validation.npz', key_prefix=prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with our data set. Of course, in real life, much more work would be needed for data cleaning and preparation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with On Demand instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a managed training job for 'mnist_keras_tf.py', \n",
    "# using a single c5.2xlarge running TensorFlow 1.15 in script mode\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point=keras_script_path, \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          script_mode=True,\n",
    "                          py_version='py3'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Managed Spot Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC2 Spot Instances have long been a great cost optimization feature, and spot training is now available on SageMaker.\n",
    "This blog [post](https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/) has more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a managed training job for 'mnist_keras_tf.py', \n",
    "# using a single c5.2xlarge spot instance running TensorFlow 1.15 in script mode\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point=keras_script_path, \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          script_mode=True,\n",
    "                          py_version='py3',\n",
    "                          train_use_spot_instances=True,        # Use spot instance\n",
    "                          train_max_run=600,                    # Max training time\n",
    "                          train_max_wait=3600,                  # Max training time + spot waiting time\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the training and validation data sets stored in S3\n",
    "\n",
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 10 minutes. Please take a look at the training log. The first few lines show SageMaker preparing the managed instance. While the job is training, you can also look at metrics in the AWS console for SageMaker, and at the training log in the the AWS console for CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is complete, the trained model is saved in S3, and is now ready to be deployed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Tensorflow CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/tensorflow-cpu-optimized"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
